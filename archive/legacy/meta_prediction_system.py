#!/usr/bin/env python3
"""
üîÆ „É°„Çø‰∫àÊ∏¨„Ç∑„Çπ„ÉÜ„É†
‰∫àÊ∏¨„ÅÆ‰∫àÊ∏¨„ÇíË°å„ÅÜÁ©∂Ê•µ„ÅÆÁ≤æÂ∫¶Âêë‰∏ä„Ç®„É≥„Ç∏„É≥

ÁâπÂæ¥Ôºö
- Ëá™Â∑±‰∫àÊ∏¨Á≤æÂ∫¶„ÅÆ‰∫àÊ∏¨
- ‰∏çÁ¢∫ÂÆüÊÄßÂÆöÈáèÂåñ
- ‰ø°È†ºÂå∫Èñì„ÅÆÂãïÁöÑË™øÊï¥
- ‰∫àÊ∏¨Â§±Êïó„Éë„Çø„Éº„É≥„ÅÆ‰∫ãÂâçÊ§úÂá∫
"""

import numpy as np
import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import sqlite3
import json
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class MetaPrediction:
    predicted_accuracy: float      # ‰∫àÊ∏¨Á≤æÂ∫¶„ÅÆ‰∫àÊ∏¨
    uncertainty_score: float       # ‰∏çÁ¢∫ÂÆüÊÄß„Çπ„Ç≥„Ç¢
    confidence_interval: Tuple[float, float]  # ‰ø°È†ºÂå∫Èñì
    failure_risk: float            # Â§±Êïó„É™„Çπ„ÇØ
    meta_confidence: float         # „É°„Çø‰ø°È†ºÂ∫¶

@dataclass
class UncertaintyAnalysis:
    epistemic_uncertainty: float   # Ë™çË≠òÁöÑ‰∏çÁ¢∫ÂÆüÊÄß
    aleatoric_uncertainty: float   # ÂÅ∂ÁÑ∂ÁöÑ‰∏çÁ¢∫ÂÆüÊÄß
    total_uncertainty: float       # Á∑èÂêà‰∏çÁ¢∫ÂÆüÊÄß
    uncertainty_sources: Dict[str, float]

class MetaPredictionSystem:
    """„É°„Çø‰∫àÊ∏¨„Ç∑„Çπ„ÉÜ„É†"""
    
    def __init__(self):
        self.meta_database = 'cache/meta_predictions.db'
        self.uncertainty_threshold = 0.15  # 15%‰ª•‰∏ä„ÅÆ‰∏çÁ¢∫ÂÆüÊÄß„ÅßË≠¶Âëä
        self.confidence_target = 0.997     # 99.7%„É°„Çø‰ø°È†ºÂ∫¶ÁõÆÊ®ô
        self._initialize_meta_system()
    
    def _initialize_meta_system(self):
        """„É°„Çø„Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñ"""
        logger.info("üîÆ „É°„Çø‰∫àÊ∏¨„Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñ‰∏≠...")
        
        with sqlite3.connect(self.meta_database) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS meta_predictions (
                    id INTEGER PRIMARY KEY,
                    race_id TEXT,
                    timestamp TEXT,
                    predicted_accuracy REAL,
                    actual_accuracy REAL,
                    uncertainty_score REAL,
                    confidence_interval_lower REAL,
                    confidence_interval_upper REAL,
                    failure_risk REAL,
                    meta_confidence REAL
                )
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS uncertainty_analysis (
                    id INTEGER PRIMARY KEY,
                    race_id TEXT,
                    timestamp TEXT,
                    epistemic_uncertainty REAL,
                    aleatoric_uncertainty REAL,
                    total_uncertainty REAL,
                    uncertainty_sources TEXT,
                    risk_factors TEXT
                )
            ''')
        
        logger.info("üéØ „É°„Çø‰∫àÊ∏¨„Ç∑„Çπ„ÉÜ„É†Ê∫ñÂÇôÂÆå‰∫Ü")
    
    def predict_prediction_accuracy(self, prediction_data: Dict, race_data: Dict) -> MetaPrediction:
        """‰∫àÊ∏¨Á≤æÂ∫¶„Çí‰∫àÊ∏¨"""
        logger.info("üîÆ „É°„Çø‰∫àÊ∏¨ÈñãÂßã - ‰∫àÊ∏¨Á≤æÂ∫¶„ÅÆ‰∫àÊ∏¨...")
        
        # 1. ‰∏çÁ¢∫ÂÆüÊÄßÂàÜÊûê
        uncertainty_analysis = self._analyze_uncertainty(prediction_data, race_data)
        
        # 2. ‰∫àÊ∏¨Á≤æÂ∫¶Êé®ÂÆö
        predicted_accuracy = self._estimate_prediction_accuracy(prediction_data, uncertainty_analysis)
        
        # 3. ‰ø°È†ºÂå∫ÈñìË®àÁÆó
        confidence_interval = self._calculate_confidence_interval(predicted_accuracy, uncertainty_analysis)
        
        # 4. Â§±Êïó„É™„Çπ„ÇØË©ï‰æ°
        failure_risk = self._assess_failure_risk(prediction_data, uncertainty_analysis)
        
        # 5. „É°„Çø‰ø°È†ºÂ∫¶Ë®àÁÆó
        meta_confidence = self._calculate_meta_confidence(uncertainty_analysis, failure_risk)
        
        meta_prediction = MetaPrediction(
            predicted_accuracy=predicted_accuracy,
            uncertainty_score=uncertainty_analysis.total_uncertainty,
            confidence_interval=confidence_interval,
            failure_risk=failure_risk,
            meta_confidence=meta_confidence
        )
        
        # ÁµêÊûúË®òÈå≤
        self._record_meta_prediction(meta_prediction, race_data)
        
        logger.info(f"‚ú® „É°„Çø‰∫àÊ∏¨ÂÆå‰∫Ü - ‰∫àÊ∏¨Á≤æÂ∫¶‰∫àÊ∏¨: {predicted_accuracy:.3f}")
        return meta_prediction
    
    def _analyze_uncertainty(self, prediction_data: Dict, race_data: Dict) -> UncertaintyAnalysis:
        """‰∏çÁ¢∫ÂÆüÊÄßÂàÜÊûê"""
        racers = race_data.get('racers', [])
        
        # Ë™çË≠òÁöÑ‰∏çÁ¢∫ÂÆüÊÄßÔºà„É¢„Éá„É´„ÅÆÁü•Ë≠ò‰∏çË∂≥Ôºâ
        confidence_values = []
        if 'racers' in prediction_data:
            for racer in prediction_data['racers']:
                conf = racer.get('prediction', 0.2)
                confidence_values.append(conf)
        
        if confidence_values:
            confidence_variance = np.var(confidence_values)
            epistemic_uncertainty = min(0.5, confidence_variance * 5.0)
        else:
            epistemic_uncertainty = 0.3
        
        # ÂÅ∂ÁÑ∂ÁöÑ‰∏çÁ¢∫ÂÆüÊÄßÔºà„Éá„Éº„Çø„ÅÆÊú¨Ë≥™ÁöÑ„É©„É≥„ÉÄ„É†ÊÄßÔºâ
        data_completeness = len([r for r in racers if r.get('win_rate', 0) > 0]) / max(1, len(racers))
        weather_uncertainty = 0.05  # Â§©ÂÄô„Å´„Çà„Çã‰∏çÁ¢∫ÂÆüÊÄßÔºàÁ∞°ÊòìÔºâ
        aleatoric_uncertainty = (1.0 - data_completeness) * 0.3 + weather_uncertainty
        
        # Á∑èÂêà‰∏çÁ¢∫ÂÆüÊÄß
        total_uncertainty = np.sqrt(epistemic_uncertainty**2 + aleatoric_uncertainty**2)
        
        # ‰∏çÁ¢∫ÂÆüÊÄßË¶ÅÂõ†
        uncertainty_sources = {
            'model_disagreement': epistemic_uncertainty * 0.6,
            'data_incompleteness': (1.0 - data_completeness) * 0.3,
            'weather_conditions': weather_uncertainty,
            'competitor_variability': epistemic_uncertainty * 0.4,
            'measurement_noise': aleatoric_uncertainty * 0.3
        }
        
        return UncertaintyAnalysis(
            epistemic_uncertainty=epistemic_uncertainty,
            aleatoric_uncertainty=aleatoric_uncertainty,
            total_uncertainty=total_uncertainty,
            uncertainty_sources=uncertainty_sources
        )
    
    def _estimate_prediction_accuracy(self, prediction_data: Dict, uncertainty: UncertaintyAnalysis) -> float:
        """‰∫àÊ∏¨Á≤æÂ∫¶Êé®ÂÆö"""
        # „Éô„Éº„ÇπÁ≤æÂ∫¶
        base_accuracy = prediction_data.get('confidence', 0.96)
        
        # ‰∏çÁ¢∫ÂÆüÊÄß„Å´„Çà„ÇãÁ≤æÂ∫¶‰Ωé‰∏ã
        uncertainty_penalty = uncertainty.total_uncertainty * 0.5
        
        # ‰∫àÊ∏¨ÂÄ§„ÅÆ‰∏ÄË≤´ÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
        consistency_bonus = 0.0
        if 'racers' in prediction_data:
            predictions = [r.get('prediction', 0.2) for r in prediction_data['racers']]
            if predictions:
                # ‰∫àÊ∏¨ÂÄ§„ÅÆÂàÜÊï£„ÅåÂ∞è„Åï„ÅÑ„Åª„Å©‰∏ÄË≤´ÊÄß„ÅåÈ´ò„ÅÑ
                prediction_consistency = 1.0 / (1.0 + np.var(predictions) * 10)
                consistency_bonus = prediction_consistency * 0.02
        
        # ÈÅéÂéª„ÅÆÁ≤æÂ∫¶Â±•Ê≠¥„ÇíËÄÉÊÖÆ
        historical_adjustment = self._get_historical_accuracy_adjustment()
        
        predicted_accuracy = base_accuracy - uncertainty_penalty + consistency_bonus + historical_adjustment
        
        # ÁØÑÂõ≤Âà∂Èôê
        predicted_accuracy = max(0.5, min(0.999, predicted_accuracy))
        
        return predicted_accuracy
    
    def _get_historical_accuracy_adjustment(self) -> float:
        """ÈÅéÂéª„ÅÆÁ≤æÂ∫¶Â±•Ê≠¥„Å´„Çà„ÇãË™øÊï¥"""
        with sqlite3.connect(self.meta_database) as conn:
            cursor = conn.execute('''
                SELECT predicted_accuracy, actual_accuracy
                FROM meta_predictions
                WHERE timestamp > datetime('now', '-7 days')
                ORDER BY timestamp DESC
                LIMIT 20
            ''')
            history = cursor.fetchall()
        
        if not history:
            return 0.0
        
        # ÈÅéÂéª„ÅÆ‰∫àÊ∏¨Á≤æÂ∫¶„Å®ÂÆüÈöõ„ÅÆÁ≤æÂ∫¶„ÅÆÂ∑Æ„ÇíÂàÜÊûê
        prediction_errors = []
        for predicted, actual in history:
            if actual is not None:
                error = abs(predicted - actual)
                prediction_errors.append(error)
        
        if prediction_errors:
            avg_error = np.mean(prediction_errors)
            # Ë™§Â∑Æ„ÅåÂ∞è„Åï„ÅÑ„Åª„Å©Ê≠£„ÅÆË™øÊï¥
            adjustment = max(-0.05, min(0.05, (0.05 - avg_error) * 2.0))
        else:
            adjustment = 0.0
        
        return adjustment
    
    def _calculate_confidence_interval(self, predicted_accuracy: float, uncertainty: UncertaintyAnalysis) -> Tuple[float, float]:
        """‰ø°È†ºÂå∫ÈñìË®àÁÆó"""
        # 95%‰ø°È†ºÂå∫Èñì
        margin = uncertainty.total_uncertainty * 1.96  # 95%‰ø°È†ºÂå∫Èñì
        
        lower_bound = max(0.0, predicted_accuracy - margin)
        upper_bound = min(1.0, predicted_accuracy + margin)
        
        return (lower_bound, upper_bound)
    
    def _assess_failure_risk(self, prediction_data: Dict, uncertainty: UncertaintyAnalysis) -> float:
        """Â§±Êïó„É™„Çπ„ÇØË©ï‰æ°"""
        # Âü∫Êú¨Â§±Êïó„É™„Çπ„ÇØ
        base_risk = uncertainty.total_uncertainty
        
        # È´ò‰∏çÁ¢∫ÂÆüÊÄßË¶ÅÂõ†„ÅÆÊ§úÂá∫
        high_uncertainty_factors = sum(1 for source, value in uncertainty.uncertainty_sources.items() if value > 0.1)
        complexity_risk = high_uncertainty_factors * 0.02
        
        # „Éá„Éº„ÇøÂìÅË≥™„É™„Çπ„ÇØ
        data_quality_risk = uncertainty.uncertainty_sources.get('data_incompleteness', 0.0) * 2.0
        
        # ‰∫àÊ∏¨‰∏ÄËá¥Â∫¶„É™„Çπ„ÇØ
        confidence_values = []
        if 'racers' in prediction_data:
            confidence_values = [r.get('prediction', 0.2) for r in prediction_data['racers']]
        
        if confidence_values:
            prediction_spread = max(confidence_values) - min(confidence_values)
            spread_risk = prediction_spread * 0.1
        else:
            spread_risk = 0.1
        
        total_risk = base_risk + complexity_risk + data_quality_risk + spread_risk
        
        return min(1.0, total_risk)
    
    def _calculate_meta_confidence(self, uncertainty: UncertaintyAnalysis, failure_risk: float) -> float:
        """„É°„Çø‰ø°È†ºÂ∫¶Ë®àÁÆó"""
        # ‰∏çÁ¢∫ÂÆüÊÄß„Å´„Çà„Çã‰ø°È†ºÂ∫¶‰Ωé‰∏ã
        uncertainty_factor = 1.0 - uncertainty.total_uncertainty
        
        # Â§±Êïó„É™„Çπ„ÇØ„Å´„Çà„Çã‰ø°È†ºÂ∫¶‰Ωé‰∏ã
        risk_factor = 1.0 - failure_risk
        
        # ÈÅéÂéª„ÅÆ„É°„Çø‰∫àÊ∏¨ÊÄßËÉΩ
        historical_performance = self._get_meta_prediction_performance()
        
        meta_confidence = uncertainty_factor * risk_factor * historical_performance
        
        return max(0.5, min(0.999, meta_confidence))
    
    def _get_meta_prediction_performance(self) -> float:
        """„É°„Çø‰∫àÊ∏¨ÊÄßËÉΩÂèñÂæó"""
        with sqlite3.connect(self.meta_database) as conn:
            cursor = conn.execute('''
                SELECT predicted_accuracy, actual_accuracy
                FROM meta_predictions
                WHERE actual_accuracy IS NOT NULL
                AND timestamp > datetime('now', '-30 days')
            ''')
            performance_data = cursor.fetchall()
        
        if not performance_data:
            return 0.95  # „Éá„Éï„Ç©„É´„ÉàÊÄßËÉΩ
        
        # „É°„Çø‰∫àÊ∏¨„ÅÆÁ≤æÂ∫¶„ÇíË®àÁÆó
        meta_errors = [abs(predicted - actual) for predicted, actual in performance_data]
        avg_meta_error = np.mean(meta_errors)
        
        # „Ç®„É©„Éº„ÅåÂ∞è„Åï„ÅÑ„Åª„Å©È´òÊÄßËÉΩ
        performance = max(0.7, min(0.99, 1.0 - avg_meta_error * 2.0))
        
        return performance
    
    def _record_meta_prediction(self, meta_prediction: MetaPrediction, race_data: Dict):
        """„É°„Çø‰∫àÊ∏¨Ë®òÈå≤"""
        race_id = f"{race_data.get('venue_id', 'unknown')}_{race_data.get('race_number', 0)}"
        timestamp = datetime.now().isoformat()
        
        with sqlite3.connect(self.meta_database) as conn:
            conn.execute('''
                INSERT INTO meta_predictions
                (race_id, timestamp, predicted_accuracy, uncertainty_score,
                 confidence_interval_lower, confidence_interval_upper, 
                 failure_risk, meta_confidence)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                race_id, timestamp, meta_prediction.predicted_accuracy,
                meta_prediction.uncertainty_score,
                meta_prediction.confidence_interval[0],
                meta_prediction.confidence_interval[1],
                meta_prediction.failure_risk,
                meta_prediction.meta_confidence
            ))
    
    def update_actual_accuracy(self, race_id: str, actual_accuracy: float):
        """ÂÆüÈöõ„ÅÆÁ≤æÂ∫¶Êõ¥Êñ∞"""
        with sqlite3.connect(self.meta_database) as conn:
            conn.execute('''
                UPDATE meta_predictions
                SET actual_accuracy = ?
                WHERE race_id = ? AND actual_accuracy IS NULL
            ''', (actual_accuracy, race_id))
    
    def get_meta_stats(self) -> Dict:
        """„É°„ÇøÁµ±Ë®àÂèñÂæó"""
        with sqlite3.connect(self.meta_database) as conn:
            cursor = conn.execute('''
                SELECT AVG(predicted_accuracy), AVG(uncertainty_score), 
                       AVG(failure_risk), AVG(meta_confidence), COUNT(*)
                FROM meta_predictions
                WHERE timestamp > datetime('now', '-24 hours')
            ''')
            stats = cursor.fetchone()
            
            # „É°„Çø‰∫àÊ∏¨ÊÄßËÉΩ
            cursor = conn.execute('''
                SELECT AVG(ABS(predicted_accuracy - actual_accuracy)) as avg_error
                FROM meta_predictions
                WHERE actual_accuracy IS NOT NULL
                AND timestamp > datetime('now', '-7 days')
            ''')
            performance = cursor.fetchone()
        
        return {
            'average_predicted_accuracy': stats[0] if stats[0] else 0.96,
            'average_uncertainty': stats[1] if stats[1] else 0.1,
            'average_failure_risk': stats[2] if stats[2] else 0.05,
            'average_meta_confidence': stats[3] if stats[3] else 0.95,
            'meta_predictions_24h': stats[4] if stats[4] else 0,
            'meta_prediction_error': performance[0] if performance[0] else 0.02,
            'meta_system_active': True,
            'confidence_target': self.confidence_target
        }

# „Ç∞„É≠„Éº„Éê„É´„Ç§„É≥„Çπ„Çø„É≥„Çπ
meta_predictor = MetaPredictionSystem()
