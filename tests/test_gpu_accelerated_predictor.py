#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUåŠ é€ŸåŒ–äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç›®æ¨™ï¼šç¾åœ¨ã®0.12ç§’ã‹ã‚‰0.01-0.05ç§’ã¸ã®10-12å€ã®å‡¦ç†é€Ÿåº¦å‘ä¸Šã‚’æ¤œè¨¼
"""

import time
import numpy as np
import pandas as pd
import torch
import psutil
import gc
from typing import Dict, Any, List
from pathlib import Path

# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from ml.gpu_accelerated_predictor import GPUAcceleratedPredictor
    from ml.realtime_inference_optimizer import RealtimeInferenceOptimizer
    from data.simple_models import TeikokuRaceInfo, TeikokuRacerInfo
except ImportError as e:
    print(f"Module import error: {e}")
    print("Required modules not found.")


class GPUPerformanceTester:
    """GPUåŠ é€ŸåŒ–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self):
        self.gpu_predictor = None
        self.cpu_predictor = None
        self.test_results = {}
        
    def setup_test_environment(self):
        """ãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("Setup test environment...")
        
        # GPUåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        if torch.cuda.is_available():
            print(f"CUDA available: {torch.cuda.get_device_name(0)}")
            print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        else:
            print("CUDA not available - CPU testing only")
            
        # äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        try:
            self.gpu_predictor = GPUAcceleratedPredictor()
            self.cpu_predictor = RealtimeInferenceOptimizer()
            print("Prediction systems initialized successfully")
        except Exception as e:
            print(f"Prediction system initialization error: {e}")
            
    def generate_test_data(self, num_races: int = 1000) -> pd.DataFrame:
        """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        print(f"Generating test data... ({num_races} races)")
        
        np.random.seed(42)
        
        # ãƒ¬ãƒ¼ã‚µãƒ¼æƒ…å ±ç”Ÿæˆ
        racers_data = []
        for race_id in range(num_races):
            for racer_num in range(1, 7):  # 1-6å·è‰‡
                racer_data = {
                    'race_id': f"race_{race_id:04d}",
                    'racer_number': racer_num,
                    'racer_name': f"é¸æ‰‹{race_id:04d}_{racer_num}",
                    'age': np.random.randint(20, 60),
                    'weight': np.random.uniform(45, 80),
                    'recent_performance': np.random.uniform(0.0, 1.0),
                    'win_rate': np.random.uniform(0.1, 0.8),
                    'course_win_rate': np.random.uniform(0.1, 0.6),
                    'motor_score': np.random.uniform(0.3, 0.9),
                    'boat_score': np.random.uniform(0.3, 0.9),
                    'weather_factor': np.random.uniform(0.8, 1.2),
                    'course_difficulty': np.random.uniform(0.7, 1.3),
                    'starting_position': racer_num,
                    'training_score': np.random.uniform(0.4, 1.0),
                    'exhibition_score': np.random.uniform(0.4, 1.0),
                    'recent_avg_time': np.random.uniform(6.8, 7.2),
                    'start_timing': np.random.uniform(-0.2, 0.2),
                    'turn_skill': np.random.uniform(0.3, 1.0),
                    'straight_speed': np.random.uniform(0.4, 1.0),
                    'experience_years': np.random.randint(1, 30),
                    'venue_experience': np.random.randint(0, 100),
                    'opponent_strength': np.random.uniform(0.3, 0.9),
                    'recent_form': np.random.uniform(0.2, 1.0),
                    'physical_condition': np.random.uniform(0.7, 1.0)
                }
                racers_data.append(racer_data)
                
        df = pd.DataFrame(racers_data)
        print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(df)}è¡Œ")
        return df
        
    def measure_prediction_speed(self, predictor, test_data: pd.DataFrame, 
                               num_iterations: int = 100) -> Dict[str, float]:
        """äºˆæ¸¬é€Ÿåº¦æ¸¬å®š"""
        print(f"â±ï¸  äºˆæ¸¬é€Ÿåº¦æ¸¬å®šä¸­... ({num_iterations}å›)")
        
        times = []
        predictions = []
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
        for _ in range(10):
            try:
                if hasattr(predictor, 'predict_gpu'):
                    _ = predictor.predict_gpu(test_data.head(6))
                else:
                    _ = predictor.predict_realtime(test_data.head(6))
            except Exception:
                pass
                
        # å®Ÿæ¸¬å®š
        for i in range(num_iterations):
            start_time = time.perf_counter()
            
            try:
                if hasattr(predictor, 'predict_gpu'):
                    prediction = predictor.predict_gpu(test_data.head(6))
                else:
                    prediction = predictor.predict_realtime(test_data.head(6))
                    
                end_time = time.perf_counter()
                
                execution_time = end_time - start_time
                times.append(execution_time)
                predictions.append(prediction)
                
            except Exception as e:
                print(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ (iteration {i}): {e}")
                continue
                
        if not times:
            return {"error": "æ¸¬å®šå¤±æ•—"}
            
        return {
            "mean_time": np.mean(times),
            "median_time": np.median(times),
            "min_time": np.min(times),
            "max_time": np.max(times),
            "std_time": np.std(times),
            "total_predictions": len(predictions),
            "success_rate": len(predictions) / num_iterations,
            "throughput_per_second": 1.0 / np.mean(times)
        }
        
    def measure_memory_usage(self, predictor, test_data: pd.DataFrame) -> Dict[str, float]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š"""
        print("ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šä¸­...")
        
        process = psutil.Process()
        
        # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        initial_gpu_memory = 0
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            initial_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
            
        # äºˆæ¸¬å®Ÿè¡Œ
        try:
            if hasattr(predictor, 'predict_gpu'):
                _ = predictor.predict_gpu(test_data.head(6))
            else:
                _ = predictor.predict_realtime(test_data.head(6))
        except Exception as e:
            print(f"ãƒ¡ãƒ¢ãƒªæ¸¬å®šä¸­ã®äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            
        # æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        final_gpu_memory = 0
        
        if torch.cuda.is_available():
            final_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
            
        return {
            "cpu_memory_initial_mb": initial_memory,
            "cpu_memory_final_mb": final_memory,
            "cpu_memory_usage_mb": final_memory - initial_memory,
            "gpu_memory_initial_mb": initial_gpu_memory,
            "gpu_memory_final_mb": final_gpu_memory,
            "gpu_memory_usage_mb": final_gpu_memory - initial_gpu_memory
        }
        
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        print("ğŸš€ åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 60)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        test_data = self.generate_test_data(1000)
        
        results = {
            "test_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "test_data_size": len(test_data),
            "cuda_available": torch.cuda.is_available(),
            "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A"
        }
        
        # GPUäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
        if self.gpu_predictor and torch.cuda.is_available():
            print("\nğŸ”¥ GPUåŠ é€ŸåŒ–äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
            print("-" * 40)
            
            # é€Ÿåº¦æ¸¬å®š
            gpu_speed_results = self.measure_prediction_speed(
                self.gpu_predictor, test_data, num_iterations=100
            )
            results["gpu_speed"] = gpu_speed_results
            
            # ãƒ¡ãƒ¢ãƒªæ¸¬å®š
            gpu_memory_results = self.measure_memory_usage(
                self.gpu_predictor, test_data
            )
            results["gpu_memory"] = gpu_memory_results
            
            print(f"   å¹³å‡å‡¦ç†æ™‚é–“: {gpu_speed_results.get('mean_time', 0):.4f}ç§’")
            print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {gpu_speed_results.get('throughput_per_second', 0):.1f} predictions/sec")
            
        # CPUäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ (æ¯”è¼ƒç”¨)
        if self.cpu_predictor:
            print("\nğŸ’» CPUäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ (æ¯”è¼ƒç”¨)")
            print("-" * 40)
            
            # é€Ÿåº¦æ¸¬å®š
            cpu_speed_results = self.measure_prediction_speed(
                self.cpu_predictor, test_data, num_iterations=50
            )
            results["cpu_speed"] = cpu_speed_results
            
            # ãƒ¡ãƒ¢ãƒªæ¸¬å®š
            cpu_memory_results = self.measure_memory_usage(
                self.cpu_predictor, test_data
            )
            results["cpu_memory"] = cpu_memory_results
            
            print(f"   å¹³å‡å‡¦ç†æ™‚é–“: {cpu_speed_results.get('mean_time', 0):.4f}ç§’")
            print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {cpu_speed_results.get('throughput_per_second', 0):.1f} predictions/sec")
            
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
        if "gpu_speed" in results and "cpu_speed" in results:
            gpu_time = results["gpu_speed"].get("mean_time", float('inf'))
            cpu_time = results["cpu_speed"].get("mean_time", float('inf'))
            
            if gpu_time > 0 and cpu_time > 0:
                speedup = cpu_time / gpu_time
                results["speedup_factor"] = speedup
                
                print(f"\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ")
                print("-" * 40)
                print(f"   GPUå‡¦ç†æ™‚é–“: {gpu_time:.4f}ç§’")
                print(f"   CPUå‡¦ç†æ™‚é–“: {cpu_time:.4f}ç§’")
                print(f"   åŠ é€Ÿç‡: {speedup:.1f}å€")
                
                # ç›®æ¨™é”æˆãƒã‚§ãƒƒã‚¯
                target_time_min = 0.01  # ç›®æ¨™æœ€å°æ™‚é–“
                target_time_max = 0.05  # ç›®æ¨™æœ€å¤§æ™‚é–“
                target_speedup_min = 10  # ç›®æ¨™åŠ é€Ÿç‡æœ€å°
                
                print(f"\nğŸ¯ ç›®æ¨™é”æˆçŠ¶æ³")
                print("-" * 40)
                print(f"   ç›®æ¨™å‡¦ç†æ™‚é–“: {target_time_min}-{target_time_max}ç§’")
                print(f"   å®Ÿéš›å‡¦ç†æ™‚é–“: {gpu_time:.4f}ç§’ {'âœ…' if target_time_min <= gpu_time <= target_time_max else 'âŒ'}")
                print(f"   ç›®æ¨™åŠ é€Ÿç‡: {target_speedup_min}å€ä»¥ä¸Š")
                print(f"   å®Ÿéš›åŠ é€Ÿç‡: {speedup:.1f}å€ {'âœ…' if speedup >= target_speedup_min else 'âŒ'}")
                
        return results
        
    def save_benchmark_results(self, results: Dict[str, Any]):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®ä¿å­˜"""
        print("\nğŸ’¾ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœä¿å­˜ä¸­...")
        
        # çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        results_dir = Path("benchmark_results")
        results_dir.mkdir(exist_ok=True)
        
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"gpu_benchmark_{timestamp}.json"
        
        import json
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
        print(f"âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœä¿å­˜å®Œäº†: {results_file}")
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_summary_report(results, results_dir / f"gpu_summary_{timestamp}.txt")
        
    def generate_summary_report(self, results: Dict[str, Any], report_path: Path):
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("ğŸš€ GPUåŠ é€ŸåŒ–äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼\n")
            f.write("=" * 70 + "\n\n")
            
            f.write(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ—¥æ™‚: {results.get('test_timestamp', 'N/A')}\n")
            f.write(f"CUDAåˆ©ç”¨å¯èƒ½: {'ã¯ã„' if results.get('cuda_available', False) else 'ã„ã„ãˆ'}\n")
            f.write(f"GPUå: {results.get('gpu_name', 'N/A')}\n")
            f.write(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {results.get('test_data_size', 0)}è¡Œ\n\n")
            
            # GPUæ€§èƒ½çµæœ
            if "gpu_speed" in results:
                gpu_speed = results["gpu_speed"]
                f.write("ğŸ”¥ GPUåŠ é€ŸåŒ–ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½\n")
                f.write("-" * 40 + "\n")
                f.write(f"å¹³å‡å‡¦ç†æ™‚é–“: {gpu_speed.get('mean_time', 0):.4f}ç§’\n")
                f.write(f"æœ€é€Ÿå‡¦ç†æ™‚é–“: {gpu_speed.get('min_time', 0):.4f}ç§’\n")
                f.write(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {gpu_speed.get('throughput_per_second', 0):.1f} predictions/sec\n")
                f.write(f"æˆåŠŸç‡: {gpu_speed.get('success_rate', 0) * 100:.1f}%\n\n")
                
            # CPUæ€§èƒ½çµæœ
            if "cpu_speed" in results:
                cpu_speed = results["cpu_speed"]
                f.write("ğŸ’» CPU ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ (æ¯”è¼ƒç”¨)\n")
                f.write("-" * 40 + "\n")
                f.write(f"å¹³å‡å‡¦ç†æ™‚é–“: {cpu_speed.get('mean_time', 0):.4f}ç§’\n")
                f.write(f"æœ€é€Ÿå‡¦ç†æ™‚é–“: {cpu_speed.get('min_time', 0):.4f}ç§’\n")
                f.write(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {cpu_speed.get('throughput_per_second', 0):.1f} predictions/sec\n")
                f.write(f"æˆåŠŸç‡: {cpu_speed.get('success_rate', 0) * 100:.1f}%\n\n")
                
            # åŠ é€Ÿç‡
            if "speedup_factor" in results:
                speedup = results["speedup_factor"]
                f.write("âš¡ åŠ é€Ÿç‡\n")
                f.write("-" * 40 + "\n")
                f.write(f"GPU vs CPU: {speedup:.1f}å€é«˜é€ŸåŒ–\n\n")
                
            # ç›®æ¨™é”æˆè©•ä¾¡
            f.write("ğŸ¯ ç›®æ¨™é”æˆè©•ä¾¡\n")
            f.write("-" * 40 + "\n")
            f.write("ç›®æ¨™: å‡¦ç†é€Ÿåº¦10-12å€å‘ä¸Š (0.12ç§’ â†’ 0.01-0.05ç§’)\n")
            
            if "gpu_speed" in results:
                gpu_time = results["gpu_speed"].get("mean_time", float('inf'))
                current_baseline = 0.12  # ç¾åœ¨ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
                actual_improvement = current_baseline / gpu_time if gpu_time > 0 else 0
                
                f.write(f"å®Ÿç¸¾: {actual_improvement:.1f}å€å‘ä¸Š ({gpu_time:.4f}ç§’)\n")
                f.write(f"ç›®æ¨™é”æˆ: {'âœ… é”æˆ' if actual_improvement >= 10 else 'âŒ æœªé”æˆ'}\n")
                
        print(f"âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ GPUåŠ é€ŸåŒ–äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 70)
    
    # ãƒ†ã‚¹ã‚¿ãƒ¼åˆæœŸåŒ–
    tester = GPUPerformanceTester()
    
    try:
        # ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        tester.setup_test_environment()
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        benchmark_results = tester.run_comprehensive_benchmark()
        
        # çµæœä¿å­˜
        tester.save_benchmark_results(benchmark_results)
        
        print("\nğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
        print("è©³ç´°ãªçµæœã¯ benchmark_results/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()